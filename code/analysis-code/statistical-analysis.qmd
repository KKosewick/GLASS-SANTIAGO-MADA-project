---
title: "Statistical Analysis"
author: "Taylor Glass and Arlyn Santiago"
date: "03/12/24"
output: html_document
---
## Load necessary packages
```{r}
#| message: false
library(dplyr) # for basic syntax
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) # for model building 
library(yardstick) # for finding performance metrics 
library(rsample) #for splitting the data into testing/training
library(knitr) # to create performance metric tables 
library(parsnip) # to create a workflow for cross-validation
```

## Load the cleaned data
```{r}
#path to data
data_location <- here("data","processed-data","processeddata.rds")
#load data
mydata <- readRDS(data_location)
```

## Data fitting and statistical analysis 
The outcome of interest in all of the following models is the percentage of unintended pregnancies associated with usage of short acting reversible contraception methods. The predictors used in our models include total number of women using short acting reversible methods standardized by population size, region, percent of women currently married, percent of women never married, total rate of maternal deaths, rate of maternal deaths due to unintended pregnancy, and rate of maternal deaths due to abortion. We are considering adding another predictor using a political freedom index for each country.

### First Model Fit 
The first model attempts to predict the percentage of unintended pregnancies among women using short acting reversible methods of contraception 'pct_upreg_sarc' based on the total number of women using this method of contraception. We will use the version of the variable that has been standardized by population size, 'sarc_standard'. 
```{r}
# fit linear model using pct_upreg_sarc as outcome, sarc_standard as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit1 <- lm_mod %>% 
            fit(pct_upreg_sarc ~ sarc_standard, mydata) #estimate/train the model 

#generate clean output with estimates and p-values
tidy(lmfit1) 

# save results from fit into a data frame 
lmtable1 <- tidy(lmfit1)

# save fit results table  
table_file1 = here("results", "tables", "resulttable1.rds")
saveRDS(lmtable1, file = table_file1)
```
There appears to be a very strong relationship between total number of women using this birth control method and the outcome based on the coefficient estimate of 87.65998.

### Second Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, region as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit2 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_larcster ~ region, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit2) 

# save results from fit into a data frame 
lmtable2 <- tidy(lmfit2)

# save fit results table  
table_file2 = here("results", "tables", "resulttable2.rds")
saveRDS(lmtable2, file = table_file2)

```
This model shows that the Latin America/the Caribbean region has the strongest association with the outcome variable based on a coefficient estimate of 1.6218844 followed closely by the Asia region. The Oceania region has the weakest association with the outcome variable based on a coefficient estimate of 0.2512100.

### ANOVA for region variable
We want to compare the mean percent of unintended pregnancies among those using short acting reversible methods across the five regions: Asia, Africa, Europe, Latin America and the Caribbean, and Oceania. We will start with an ANOVA test to determine the relationship between region and outcome. We are more interested in pairwise comparisons between the regions, so we also conducted Tukey's Honestly Significant Difference test for pairwise comparisons within ANOVA. 
```{r}
# conduct an ANOVA test with the region variable
regionANOVA <- aov(pct_upreg_sarc ~ region, data = mydata)

# explore ANOVA results 
summary(regionANOVA)

# add Tukey's HSD test for pairwise comparisons
tukey_result <- TukeyHSD(regionANOVA)

# explore the pairwise comparisons
tidy(tukey_result)

# save fit results table  
table_file13 = here("results", "tables", "resulttable13.rds")
saveRDS(tukey_result, file = table_file13)
```
ANOVA results show that the region variable is statistically significantly associated with the outcome variable given the p-value less than 0.001. The Tukey HSD test revealed the largest differences in the outcome between the Oceania region and Latin America/the Caribbean, Europe, and Asia. The relationships between Oceania and Europe and Latin America/the Caribbean are statistically significant with p-values less than 0.0001. Oceania is clearly experiencing the lowest percentage of unintended pregnancies among short acting reversible method users.  The lowest difference in outcome between regions is between Asia and Africa, but this is not a statistically significant association. Two statistically significant differences in the outcome exist between Latin America/the Caribbean and Europe with Africa, which shows that Africa is experiencing higher levels of the outcome. 

### Third Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, pct_currentlymarried as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit3 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ pct_currentlymarried, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit3) 

# save results from fit into a data frame 
lmtable3 <- tidy(lmfit3)

# save fit results table  
table_file3 = here("results", "tables", "resulttable3.rds")
saveRDS(lmtable3, file = table_file3)
```
This model shows there is a negative relationship between the number of women currently married and the outcome. As the percentage of currently married women increases by 1, the percentage of unintended pregnancies decreases by a factor of 0.4181571.

# Fourth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, pct_nevermarried as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit4 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ pct_nevermarried, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit4) 

# save results from fit into a data frame 
lmtable4 <- tidy(lmfit4)

# save fit results table  
table_file4 = here("results", "tables", "resulttable4.rds")
saveRDS(lmtable4, file = table_file4)
```
This model shows a positive relationship between the percentage of women never married and the outcome. As the percentage of women never married increases by 1, the percentage of unintended pregnancies increases by a factor of 0.2474218.  

# Fifth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit5 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit5) 

# save results from fit into a data frame 
lmtable5 <- tidy(lmfit5)

# save fit results table  
table_file5 = here("results", "tables", "resulttable5.rds")
saveRDS(lmtable5, file = table_file5)
```
This model shows that total rate of maternal deaths is slightly negatively correlated with the outcome based on a small coefficient estimate of -0.01482568. 

# Sixth Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths_upreg as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit6 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths_upreg, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit6) 

# save results from fit into a data frame 
lmtable6 <- tidy(lmfit6)

# save fit results table  
table_file6 = here("results", "tables", "resulttable6.rds")
saveRDS(lmtable6, file = table_file6)
```
This model shows that total rate of maternal deaths due to unintended pregnancies has an even weaker association with the outcome based on a small coefficient estimate of -0.003158542. 

# Seventh Model Fit 
```{r}
# fit linear model using pct_upreg_sarc as outcome, rate_matdeaths_abs as predictor
lm_mod <- linear_reg() #specify the type of model
lmfit7 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit7) 

# save results from fit into a data frame 
lmtable7 <- tidy(lmfit7)

# save fit results table  
table_file7 = here("results", "tables", "resulttable7.rds")
saveRDS(lmtable7, file = table_file7)
```
This model shows that total rate of maternal deaths due to abortion has the strongest association with the outcome out of the three maternal mortality rates included based on a coefficient estimate of -0.033353527.

### Assessing correlation between predictors
There are some variables that are intuitively correlated, which means they should not be included in the same model. We want to explore those correlations before building our multivariate linear regression model. 
```{r}
# find correlation coefficients 
cor(mydata$pct_currentlymarried, mydata$pct_nevermarried)
cor(mydata$rate_matdeaths, mydata$rate_matdeaths_upreg)
cor(mydata$rate_matdeaths, mydata$rate_matdeaths_abs)
cor(mydata$rate_matdeaths_upreg, mydata$rate_matdeaths_abs)
```
The percent of currently married women and the percent of never married women have a correlation coefficient of -0.8588, so these two variables are highly correlated. The total rate of maternal deaths and rate of maternal deaths due to unintended pregnancies is the most highly correlated pair with a coefficient of 0.8904. The correlation between total rate of maternal deaths and rate of maternal deaths due to abortion is slightly lower with a coefficient of 0.8023. The two variables with the least correlation out of these four pairs are rate of maternal deaths due to unintended pregnancies and rate of maternal deaths due to abortion with a coefficient of 0.6486. Based on these associations, we will only include the percent of currently married women in the multivariate model because it has a slightly larger effect on the outcome with a more significant p-value in its single predictor model compared to the percent of never married women predictor. We will include both the rate of maternal deaths due to unintended pregnancies and rate of maternal deaths due to abortion because the total rate of maternal deaths is so highly correlated with each of these variables.

### Eighth Model Fit 
Based on the exploration of correlation between predictors, we include 5 predictors in the multiple linear regression model to determine how the associations interact. 
```{r}
# fit linear model using pct_upreg_sarc as outcome with all 7 predictors
lm_mod <- linear_reg() #specify the type of model
lmfit8 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg + rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit8) 

# save results from fit into a data frame 
lmtable8 <- tidy(lmfit8)

# save fit results table  
table_file8 = here("results", "tables", "resulttable8.rds")
saveRDS(lmtable8, file = table_file8)
```
This model shows the strongest predictor is the number of women using short acting reversible contraception with a huge coefficient of 85.6433. The next strongest predictor is the European region followed by the Latin America/the Caribbean region and the Asia region, which is a shift from the model with the singular region predictor. The Oceania region now has a negative association with the outcome. Both of the marriage status predictors are associated with a decrease in the outcome. The coefficient estimates for the rate of maternal death due to unintended pregnancies and abortion both flipped from the single predictor models to show a positive association with the outcome. 

### Nineth model fit 
It is important to create the null model for future assessment of model performance, which just calculates the mean outcome without any predictors present. 
```{r}
# create a null model
null_model <- null_model(mode = "regression") %>% 
    set_engine("parsnip") %>%
    fit(pct_upreg_sarc ~ 1, data = mydata)

#generate clean output with estimates and p-values
tidy(null_model) 

# save results from fit into a data frame 
nulltable <- tidy(null_model)

# save fit results table  
table_file9 = here("results", "tables", "resulttable9.rds")
saveRDS(nulltable, file = table_file9)
```
We also want to observe models with different combinations of predictors to determine if there is any evidence of interaction or effect measure modification. Multivariate analysis allows for the assessment of the association between each predictor and the outcome while adjusting for the effects of other variables. 

### Tenth Model fit 
We will create a model with all 3 rate of maternal mortality predictors to compare to the three models with each predictor on its own to further explore the correlation between these variables. 
```{r}
# fit linear model using pct_upreg_sarc as outcome with rate of maternal mortality predictors
lm_mod <- linear_reg() #specify the type of model
lmfit10 <- lm_mod %>% #estimate/train the model 
            fit(pct_upreg_sarc ~ rate_matdeaths + rate_matdeaths_upreg + rate_matdeaths_abs, mydata)      

#generate clean output with estimates and p-values
tidy(lmfit10) 

# save results from fit into a data frame 
lmtable10 <- tidy(lmfit10)

# save fit results table  
table_file10 = here("results", "tables", "resulttable10.rds")
saveRDS(lmtable10, file = table_file10)
```
It appears that there is an association between these three variables because the direction of the association between rate of maternal deaths and outcome has flipped. This model shows that rate of maternal deaths is associated with an increase in the outcome by a factor of 0.011267, while the single predictor model showed a slightly negatively correlation with the outcome based on a small coefficient estimate of -0.01482568. The other two variables retain their negative association with the outcome variable with similar coefficient measurements. 

## Assessing Model Performance
It is important to consider appropriate metrics to measure model performance. We will use RMSE and MAE at this point to compare 3 models: one with the number of women using short acting reversible contraception predictor, one using the percent never married predictor, and one using the rate of maternal deaths due to abortion predictor. We chose these predictors because they have the strongest associations with the outcomes based on the single linear regression models shown above. We chose these performance metrics because they are commonly used for linear regression models like the ones we have created. RMSE provides an average magnitude of the error in the predictors, and MAE provides an average absolute difference between the predicted and actual values. We include both metrics to increase the validity of our results. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the sarc_standard predictor model
poppred <- predict(lmfit1, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
popdata <- bind_cols(poppred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse1 <- rmse(popdata, truth = pct_upreg_sarc, estimate = .pred)
mae1 <- mae(popdata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the pct_curmarried predictor model
marriedpred <- predict(lmfit3, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
marrieddata <- bind_cols(marriedpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse2 <- rmse(marrieddata, truth = pct_upreg_sarc, estimate = .pred)
mae2 <- mae(marrieddata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the rate of maternal deaths due to abortion predictor model
matdeaths_abspred <- predict(lmfit7, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeath_absdata <- bind_cols(matdeaths_abspred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse3 <- rmse(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)
mae3 <- mae(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the null model
nullpred <- predict(lmfit9, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
nulldata <- bind_cols(nullpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse4 <- rmse(nulldata, truth = pct_upreg_sarc, estimate = .pred)
mae4 <- mae(matdeath_absdata, truth = pct_upreg_sarc, estimate = .pred)


# create a matrix with RMSE and MAE values
results <- as.table(matrix(c(rmse1$.estimate, mae1$.estimate, rmse2$.estimate, mae2$.estimate, rmse3$.estimate, mae3$.estimate,
                             rmse4$.estimate, mae4$.estimate), nrow = 4, ncol = 2, byrow = TRUE))

# add column names
colnames(results) <- c("RMSE", "MAE")

# add row names
rownames(results) <- c("Model 1-Population Size Using SARCs", "Model 2-Currently Married", 
                             "Model 3-Rate of Maternal Deaths(Abortion)", "Model 4-Null")

# print the table
kable(results)

# save performance metrics results table  
table_file11 = here("results", "tables", "resulttable11.rds")
saveRDS(results, file = table_file11)
```

Both metrics favor the model with the total number of women using short acting reversible contraception methods based on the lowest RMSE value of 5.143319 and lowest MAE value of 3.953767. The model with the percentage of women currently married performs worse with an RMSE value of 10.17591 and MAE value of 8.107884. The same conclusion can be drawn for the model with the rate of maternal mortality due to abortions predictor only, given the RMSE value of 10.14128 and MAE value of 8.21687. The null model has an RMSE value of 9.842267 and MAE of 8.21687, so all models should have performance metrics lower than these values to be considered useful. 

It is important to determine if our model with the region predictor performs well because the main question to be answered by this analysis depends on this predictor. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the sarc_standard predictor model
regionpred <- predict(lmfit2, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
regiondata <- bind_cols(regionpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse5 <- rmse(regiondata, truth = pct_upreg_sarc, estimate = .pred)
mae5 <- mae(regiondata, truth = pct_upreg_sarc, estimate = .pred)
print(rmse5)
print(mae5)
```

The results of these performance metrics are disappointing because 18.11857 is much larger than the null model's RMSE of 9.842267 and 14.66513 is also larger than the null model's MAE. However, this issue could possibly be explained by the fact that Africa has the most significant differences in our outcome compared to the other regions. We will still include this predictor in our multivariate regression model based on the significant Tukey conclusions and the large differences in outcome displayed in our exploratory data analysis results. 

After examining the relationship between the rate of maternal mortality variables, we want to determine which model performs the best. We have already found the RMSE value for the rate of maternal deaths due to abortion, which is stored as `rmse3`. We need to find the RMSE values for the other two single predictor models and the model with all 3 predictors.  
```{r}
# create predictions using the rate of maternal deaths predictor model
matdeathsallpred <- predict(lmfit5, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeathalldata <- bind_cols(matdeathsallpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse6 <- rmse(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)
mae6 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)

# create predictions using the rate of maternal deaths from unintended pregnancies predictor model
matdeaths_upregpred <- predict(lmfit6, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
matdeath_upregdata <- bind_cols(matdeaths_upregpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse7 <- rmse(matdeath_upregdata, truth = pct_upreg_sarc, estimate = .pred)
mae7 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)


# create predictions using model with all three rate predictors
totalratepred <- predict(lmfit10, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
totalratedata <- bind_cols(totalratepred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse8 <- rmse(totalratedata, truth = pct_upreg_sarc, estimate = .pred)
mae8 <- mae(matdeathalldata, truth = pct_upreg_sarc, estimate = .pred)

# create a matrix with RMSE and MAE values
results2 <- as.table(matrix(c(rmse6$.estimate, mae6$.estimate, rmse7$.estimate, mae7$.estimate, rmse3$.estimate, mae3$.estimate,
                              rmse8$.estimate, mae8$.estimate, rmse4$.estimate, mae4$.estimate), nrow = 5, ncol = 2, byrow = TRUE))

# add column names
colnames(results2) <- c("RMSE", "MAE")

# add row names
rownames(results2) <- c("Maternal Morality Model", "Maternal Mortality due to Unintended Pregnancies Model", 
                        "Maternal Mortality due to Abortion Model", "All 3 Maternal Mortality Predictors Model", "Null Model")

# print the table
kable(results2)

# save performance metrics results table  
table_file12 = here("results", "tables", "resulttable12.rds")
saveRDS(results2, file = table_file12)

```
The performance metrics show that the three single predictor models perform worse than the null model, which is not ideal. The model with all 3 maternal morality rate predictors performs exactly the same as the null model, which makes sense because the coefficients estimating the effect of each predictor on the outcome are so small. There is more variation in RMSE values compared to MAE values, but the conclusions are the same. 

We want to see how the model with all 5 predictors performs in comparison to the single predictor models. 
```{r}
# set seed for reproducibility 
set.seed(2468)

# create predictions using the multiple linear regression model
allpred <- predict(lmfit8, new_data = mydata) %>% 
                  select(.pred)

# create a data frame with the predictions and true values 
alldata <- bind_cols(allpred, mydata$pct_upreg_sarc) %>%
            rename(pct_upreg_sarc = "...2")

# find performance metrics to determine model fit
rmse9 <- rmse(alldata, truth = pct_upreg_sarc, estimate = .pred)
mae9 <- mae(alldata, truth = pct_upreg_sarc, estimate = .pred)
print(rmse9)
print(mae9)
```
As expected, this model performs better than the single predictor models with the lowest RMSE score of 4.361363 and lowest MAE score of 3.386193. 

## Cross-Validation to Assess Model Performance
While the RMSE and MAE performance metrics are a good place to start for evaluating model performance, we need to determine how well the model performs on data it has not seen before to determine its usefulness. We will use the cross-validation process to do this. The first step is to split the data into testing and training portions. 
```{r}
# set seed for reproducibility
set.seed(2468)

# put 75% into the training data
datasplit <- initial_split(mydata, prop = 3/4)

# create data frames for the two sets
train_data <- training(datasplit)
test_data <- testing(datasplit)
```

We will only cross-validate the single predictor model with the highest performance, `lmfit1`, with the total population size using short acting reversible contraception and the  multivariate regression model,`lmfit8`, because it performs the best in terms of RMSE and MAE. Our dataset is relatively small with only 132 observations, so we will use 5 folds for the cross-validation process.  
```{r}
# set seed for reproducibility
set.seed(2468)

# create 10 folds
folds <- vfold_cv(train_data, v=5)

# define the model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm")


# initialize a workflow and fit it to 'lmfit1'
my_workflow <- workflow() %>%
  add_model(lm_spec) %>% 
  add_formula(pct_upreg_sarc ~ sarc_standard) %>% #specify the formula since we are not using a recipe
  fit(data = train_data) 

# resample the model 
my_resample <- my_workflow %>% 
                fit_resamples(folds)

# initialize a workflow and fit it to 'lmfit8'
my_workflow2 <- workflow() %>%
  add_model(lm_spec) %>% 
  add_formula(pct_upreg_sarc ~ sarc_standard + region + pct_currentlymarried + rate_matdeaths_upreg + rate_matdeaths_abs) %>% #specify the formula since we are not using a recipe
  fit(data = train_data) 

# resample the model 
my_resample2 <- my_workflow2 %>% 
                fit_resamples(folds)

# collect the metrics from both models
collect_metrics(my_resample)
collect_metrics(my_resample2)
```
The cross-validated results lead to the same conclusion: the multiple linear regression model with 5 predictors performs better. The RMSE value for the single predictor model with `sarc_standard` is 4.822226, which is lower the the first RMSE value of 5.143319. While the RMSE value for the multiple linear regression model is slightly higher than the first time it was calculated, it is still slightly lower at 4.405716. Additionally, the standard error the multiple linear regression model is 0.2768, which is lower than the standard error for the single predictor model of 0.3528. This means there is higher variance in the single predictor model. The R-squared value of 0.8206 for the multiple linear regression model is higher than 0.7861 for the single predictor model, which means the multiple linear regression model accounts for more of the variability in the data. Overall, the multiple linear regression model performs better.

Since we have concluded that the model with all 5 predictors performs the best, we will use this model to make predictions for the test data. We will create a graph to show how well the model performed on the training and testing data. 
```{r}
# set seed for reproducibility
set.seed(2468)

# create predictions using MLR model with the test data
testpred <- predict(my_workflow2, new_data = test_data) %>% 
                  select(.pred)

# create predictions using MLR model with training data
trainpred <- predict(my_workflow2, new_data = train_data) %>% 
                  select(.pred)

# create data frame with the observed values predicted values from model 2
plotdata <- data.frame(
  observed = c(train_data$pct_upreg_sarc), 
  training_model2 = c(trainpred$.pred), 
  test_model2 = c(testpred$.pred),
  model = rep(c("Observed", "Training Model", "Testing Model"), each = nrow(train_data))) # add label indicating the model

# create a visual representation with 2 sets of predicted and observed values 
ggplot(plotdata, aes(x = observed)) +
  geom_point(aes(y = training_model2, color = "training model"), shape = 3) +
  geom_point(aes(y = test_model2, color = "testing model"), shape = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") + #add the 45 degree line
  xlim(0, 50) + #limit the x-axis values
  ylim(0, 50) + #limit the y-axis values
  labs(x = "Observed Values", y = "Predicted Values", title = "Comparing Training and Testing Data on MLR Model")
```
The training model predictions, shown in teal, hover pretty closely around the expected relationship between the outcome and the 5 predictors, which is depicted as the 45 degree line. The testing model predictions, shown in coral, are much more extreme than the training model predictions because they are farther from the 45 degree line. While there is some overlap between the two sets of predictions, it is likely that our model is overfitting the data. 